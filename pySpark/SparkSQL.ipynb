{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = r'C:\\Users\\Marcos\\Documents\\Spark'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DataFrameSQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = 'data/walmart-sales-dataset-of-45stores.csv'\n",
    "df = spark.read.csv(path_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Holiday_Flag: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- CPI: double (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      "\n",
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "|Store|      Date|Weekly_Sales|Holiday_Flag|Temperature|Fuel_Price|        CPI|Unemployment|\n",
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "|    1|05-02-2010|   1643690.9|           0|      42.31|     2.572|211.0963582|       8.106|\n",
      "|    1|12-02-2010|  1641957.44|           1|      38.51|     2.548|211.2421698|       8.106|\n",
      "|    1|19-02-2010|  1611968.17|           0|      39.93|     2.514|211.2891429|       8.106|\n",
      "|    1|26-02-2010|  1409727.59|           0|      46.63|     2.561|211.3196429|       8.106|\n",
      "|    1|05-03-2010|  1554806.68|           0|       46.5|     2.625|211.3501429|       8.106|\n",
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('myTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "|Store|      Date|Weekly_Sales|Holiday_Flag|Temperature|Fuel_Price|        CPI|Unemployment|\n",
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "|    3|12-02-2010|   420728.96|           1|      47.93|     2.548|214.5747916|       7.368|\n",
      "|    6|12-02-2010|  1606283.86|           1|      40.57|     2.548|212.7700425|       7.259|\n",
      "|    8|12-02-2010|    994801.4|           1|      33.34|     2.548|214.6214189|       6.299|\n",
      "|    9|12-02-2010|   552677.48|           1|      37.08|     2.548|214.8056534|       6.415|\n",
      "|   11|12-02-2010|  1574684.08|           1|      48.01|     2.548|214.5747916|       7.368|\n",
      "+-----+----------+------------+------------+-----------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT * FROM myTable WHERE Date = '12-02-2010' and CPI > 212\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Store|     Average_Sales|\n",
      "+-----+------------------+\n",
      "|   20|2107676.8703496507|\n",
      "|    4|2094712.9606993007|\n",
      "|   14| 2020978.400979021|\n",
      "|   13| 2003620.306293707|\n",
      "|    2|1925751.3355244761|\n",
      "|   10| 1899424.572657342|\n",
      "|   27| 1775216.201958042|\n",
      "|    6|1564728.1862937063|\n",
      "|    1|1555264.3975524479|\n",
      "|   39| 1450668.129160839|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT Store, AVG(Weekly_Sales) AS Average_Sales FROM myTable GROUP BY Store ORDER BY Average_Sales DESC\")\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_exists = spark.catalog.tableExists('myTable')\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('myTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = 'data/walmart-sales-dataset-of-45stores.csv'\n",
    "df = spark.read.csv(path_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop('Date', 'Temperature', 'Fuel_Price', 'CPI')\n",
    "df2 = df.drop('Holiday_Flag', 'Unemployment', 'Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('sales')\n",
    "df2.createOrReplaceTempView('schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+------------+\n",
      "|Store|Weekly_Sales|Holiday_Flag|Unemployment|\n",
      "+-----+------------+------------+------------+\n",
      "|    1|   1643690.9|           0|       8.106|\n",
      "+-----+------------+------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----+----------+-----------+----------+-----------+\n",
      "|Store|      Date|Temperature|Fuel_Price|        CPI|\n",
      "+-----+----------+-----------+----------+-----------+\n",
      "|    1|05-02-2010|      42.31|     2.572|211.0963582|\n",
      "+-----+----------+-----------+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(1)\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Weekly_Sales|\n",
      "+------------+\n",
      "|   890689.51|\n",
      "|   656988.64|\n",
      "|   841264.04|\n",
      "|   741891.65|\n",
      "|   777951.22|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "                   SELECT Weekly_Sales FROM sales\n",
    "                   WHERE Store IN (\n",
    "                   SELECT Temperature FROM schedule\n",
    "                   WHERE Temperature > 43\n",
    "                   )\n",
    "                   \"\"\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+------------+----------+\n",
      "|Store|Weekly_Sales|Holiday_Flag|Unemployment|Fuel_Price|\n",
      "+-----+------------+------------+------------+----------+\n",
      "|    1|   1643690.9|           0|       8.106|     3.506|\n",
      "|    1|   1643690.9|           0|       8.106|     3.594|\n",
      "|    1|   1643690.9|           0|       8.106|     3.601|\n",
      "|    1|   1643690.9|           0|       8.106|     3.617|\n",
      "|    1|   1643690.9|           0|       8.106|     3.666|\n",
      "+-----+------------+------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "                   SELECT sales.*, schedule.Fuel_Price FROM sales\n",
    "                   JOIN schedule ON sales.Store = schedule.Store\n",
    "                   \"\"\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('Weekly_Sales').orderBy(F.desc('Fuel_Price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+------------+----------+----+\n",
      "|Store|Weekly_Sales|Holiday_Flag|Unemployment|Fuel_Price|rank|\n",
      "+-----+------------+------------+------------+----------+----+\n",
      "|   33|   209986.25|           0|       9.265|     4.468|   1|\n",
      "|   33|   209986.25|           0|       9.265|     4.449|   2|\n",
      "|   33|   209986.25|           0|       9.265|     4.308|   3|\n",
      "|   33|   209986.25|           0|       9.265|     4.301|   4|\n",
      "|   33|   209986.25|           0|       9.265|     4.294|   5|\n",
      "+-----+------------+------------+------------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.withColumn('rank', F.rank().over(window_spec)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

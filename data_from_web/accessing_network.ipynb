{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code is originated from the course \"Using Python to Access Web Data\", from professor Charles Severence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Thu, 02 Feb 2023 17:23:30 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "# mysock.connect(('desentupidorafoguete.com.br', 80))\n",
    "\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "# cmd = 'GET https://desentupidorafoguete.com.br/desentupidora-em-campinas HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "n = 0\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    n = len(words)\n",
    "    n += n\n",
    "    print(line.decode().strip())\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "n = 0\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.get('what',2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://data.pr4e.org/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/page1.htm')\n",
    "\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#main-content\n",
      "/prospective-students\n",
      "/current-students\n",
      "/employers\n",
      "/host-student-project\n",
      "/alumni\n",
      "/giving\n",
      "https://intranet.si.umich.edu/\n",
      "https://www.si.umich.edu/cse?keys\n",
      "/\n",
      "/programs\n",
      "/programs/bachelor-science-information\n",
      "/programs/master-science-information\n",
      "/programs/master-health-informatics\n",
      "/programs/master-applied-data-science\n",
      "/programs/phd-information\n",
      "/programs/undergraduate-minors\n",
      "/programs/non-degree-offerings\n",
      "/programs/courses\n",
      "/research\n",
      "/research/research-areas\n",
      "/research/student-research\n",
      "/research/funded-research-projects\n",
      "/research/research-centers-and-groups\n",
      "/research/community-impact-research\n",
      "/student-experience\n",
      "/student-experience/why-study-information\n",
      "/student-experience/academic-success\n",
      "/student-experience/career-development\n",
      "/student-experience/career-outcomes\n",
      "/student-experience/engaged-learning\n",
      "/student-experience/internships\n",
      "/student-experience/student-life\n",
      "/student-experience/life-ann-arbor\n",
      "/about-umsi\n",
      "/about-umsi/deans-welcome\n",
      "/about-umsi/diversity-equity-inclusion-office\n",
      "/about-umsi/history-mission-and-goals\n",
      "/about-umsi/fast-facts\n",
      "/about-umsi/umsi-covid-19-updates-and-resources\n",
      "/about-umsi/news\n",
      "/about-umsi/events\n",
      "/about-umsi/newsletters\n",
      "/about-umsi/getting-umsi\n",
      "/about-umsi/contact-us\n",
      "/people\n",
      "/people/directory\n",
      "/people/leadership-team\n",
      "/people/faces-umsi\n",
      "/people/umsi-advisory-board\n",
      "/people/distinguished-umsi-alumni\n",
      "/people/faculty-recruiting\n",
      "/people/student-organization-representatives\n",
      "/people/umsi-phds-job-market\n",
      "https://myumi.ch/5WqGV\n",
      "https://myumi.ch/5WqGV\n",
      "/about-umsi/news/mads-team-designs-new-data-visualization-help-save-great-lakes\n",
      "/about-umsi/news/mads-team-designs-new-data-visualization-help-save-great-lakes\n",
      "/about-umsi/news/tea-medium-understanding-social-change\n",
      "/about-umsi/news/tea-medium-understanding-social-change\n",
      "/about-umsi/news/automotive-ux-course-umsi-fueled-graduates-entering-industry\n",
      "/about-umsi/news/automotive-ux-course-umsi-fueled-graduates-entering-industry\n",
      "https://myumi.ch/P16Db\n",
      "https://myumi.ch/P16Db\n",
      "/about-umsi/news/robot-im-sorry-human-i-dont-care-anymore\n",
      "/about-umsi/news/pal-free-speech-absolutist-elon-musk-isnt-so-free-india\n",
      "/about-umsi/news/could-dislike-button-reduce-online-toxicity-umsi-researchers-say-yes\n",
      "/about-umsi/news\n",
      "/about-umsi/events/data-science/computational-social-science-seminar-nathan-teblunthuis\n",
      "/about-umsi/events/using-design-and-art-create-equitable-ai\n",
      "/about-umsi/events\n",
      "https://www.facebook.com/uomsi/\n",
      "https://www.instagram.com/umschoolofinformation\n",
      "https://twitter.com/umsi\n",
      "https://www.youtube.com/user/schoolofinformation\n",
      "https://www.linkedin.com/school/university-of-michigan---school-of-information/\n",
      "/about-umsi/contact-us\n",
      "/privacy-statement\n",
      "https://umich.qualtrics.com/jfe/form/SV_9RAiBJITFVOxkHz\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url = input('Enter - ')\n",
    "url = 'http://www.si.umich.edu'\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href',None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "x = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dr-chuck.com']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '<p>Please click <a href=\"http://www.dr-chuck.com\">here</a></p>'\n",
    "import re\n",
    "\n",
    "re.findall('href=\"(.+)\"',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2646\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url = input('Enter - ')\n",
    "# url = 'http://py4e-data.dr-chuck.net/comments_42.html'\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_1732593.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "n = 0\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "spans = soup('span')\n",
    "for span in spans:\n",
    "    # print(span.contents)\n",
    "    n += int(span.contents[0])\n",
    "print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Cohen.html http://py4e-data.dr-chuck.net/known_by_Montgomery.html http://py4e-data.dr-chuck.net/known_by_Aden.html\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "n = 0\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "# Retrieve all of the anchor tags\n",
    "tags1 = soup('a')\n",
    "for tag1 in tags1:\n",
    "    name1 = tag1.get('href').strip()\n",
    "    i += 1\n",
    "\n",
    "    if i == 3:\n",
    "        url = name1\n",
    "        html = urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        tags2 = soup('a')\n",
    "        \n",
    "        for tag2 in tags2:\n",
    "            name2 = tag1.get('href').strip()\n",
    "            j += 1\n",
    "            \n",
    "            if j == 3:\n",
    "                url = name2\n",
    "                html = urlopen(url, context=ctx).read()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                tags3 = soup('a')\n",
    "\n",
    "                for tag3 in tags3:\n",
    "                    name3 = tag3.get('href').strip()\n",
    "                    j += 1\n",
    "                    \n",
    "                    if k == 3:\n",
    "                        url = name3\n",
    "                        html = urlopen(url, context=ctx).read()\n",
    "                        soup = BeautifulSoup(html, \"html.parser\")\n",
    "                        tags4 = soup('a')\n",
    "\n",
    "                        for tag4 in tags4:\n",
    "                            name4 = tag4.get('href').strip()\n",
    "                            j += 1\n",
    "                            \n",
    "                            if k == 3:\n",
    "                                url = name4\n",
    "                                html = urlopen(url, context=ctx).read()\n",
    "                                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                                break\n",
    "print(name1,name2,name3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carson', 'Stewart', 'Bret', 'Coupar', 'Leonard', 'Monty', 'Ashlee', 'Kennedi']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Carson.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "\n",
    "n = 1\n",
    "# names = ['Fikret']\n",
    "names = ['Carson']\n",
    "\n",
    "while n <= 7:\n",
    "    i = 0\n",
    "    for tag in tags:\n",
    "        url = tag.get('href').strip()\n",
    "        name = re.findall('known_by_(.+)\\.', url)[0]\n",
    "        i += 1\n",
    "        if i == 18:\n",
    "            names.append(name)\n",
    "            break\n",
    "    \n",
    "    # url = name\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tags = soup('a')\n",
    "    n += 1\n",
    "    \n",
    "print(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "037335531d7621c1b1b0f2b1596a260ebe03ba48a827428118e6a4217105d671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
